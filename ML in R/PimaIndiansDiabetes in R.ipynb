{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare problem\n",
    "- Load libraries\n",
    "- Load Dataset\n",
    "- Split-out validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"mlbench\")\n",
    "library(mlbench)\n",
    "data(package=\"mlbench\")\n",
    "data(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Data\n",
    "\n",
    "### Understand Data with Descriptive Statistics\n",
    "\n",
    "- Understand your data using the head() function to look at the first few rows.\n",
    "- Review the distribution of your data with the summary() function.\n",
    "- Review the dimensions of your data with the dim() function.\n",
    "- Calculate pair-wise correlation between your variables using the cor() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n",
    "1        6     148       72      35       0 33.6    0.627  50      pos\n",
    "2        1      85       66      29       0 26.6    0.351  31      neg\n",
    "3        8     183       64       0       0 23.3    0.672  32      pos\n",
    "4        1      89       66      23      94 28.1    0.167  21      neg\n",
    "5        0     137       40      35     168 43.1    2.288  33      pos\n",
    "6        5     116       74       0       0 25.6    0.201  30      neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "   pregnant         glucose         pressure         triceps     \n",
    " Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n",
    " 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n",
    " Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n",
    " Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n",
    " 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n",
    " Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n",
    "    insulin           mass          pedigree           age        diabetes \n",
    " Min.   :  0.0   Min.   : 0.00   Min.   :0.0780   Min.   :21.00   neg:500  \n",
    " 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437   1st Qu.:24.00   pos:268  \n",
    " Median : 30.5   Median :32.00   Median :0.3725   Median :29.00            \n",
    " Mean   : 79.8   Mean   :31.99   Mean   :0.4719   Mean   :33.24            \n",
    " 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262   3rd Qu.:41.00            \n",
    " Max.   :846.0   Max.   :67.10   Max.   :2.4200   Max.   :81.00 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(PimaIndiansDiabetes)\n",
    "[1] 768   9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Data with Visualization\n",
    "\n",
    "- Use the hist() function to create a histogram of each attribute.\n",
    "- Use the boxplot() function to create box and whisker plots of each attribute.\n",
    "- Use the pairs() function to create pair-wise scatterplots of all attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare For Modeling by Pre-Processing Data\n",
    "\n",
    "- Standardize numerical data (e.g. mean of 0 and standard deviation of 1) using the scale and center options.\n",
    "- Normalize numerical data (e.g. to a range of 0-1) using the range option.\n",
    "- Explore more advanced power transforms like the Box-Cox power transform with the BoxCox option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"caret\")\n",
    "# load caret package\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(PimaIndiansDiabetes[,1:8], method=c(\"range\"))\n",
    "# transform the dataset using the pre-processing parameters\n",
    "transformed <- predict(preprocessParams, PimaIndiansDiabetes[,1:8])\n",
    "# summarize the transformed dataset\n",
    "summary(transformed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pregnant          glucose          pressure         triceps      \n",
    " Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n",
    " 1st Qu.:0.05882   1st Qu.:0.4975   1st Qu.:0.5082   1st Qu.:0.0000  \n",
    " Median :0.17647   Median :0.5879   Median :0.5902   Median :0.2323  \n",
    " Mean   :0.22618   Mean   :0.6075   Mean   :0.5664   Mean   :0.2074  \n",
    " 3rd Qu.:0.35294   3rd Qu.:0.7048   3rd Qu.:0.6557   3rd Qu.:0.3232  \n",
    " Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n",
    "    insulin             mass           pedigree            age        \n",
    " Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n",
    " 1st Qu.:0.00000   1st Qu.:0.4069   1st Qu.:0.07077   1st Qu.:0.0500  \n",
    " Median :0.03605   Median :0.4769   Median :0.12575   Median :0.1333  \n",
    " Mean   :0.09433   Mean   :0.4768   Mean   :0.16818   Mean   :0.2040  \n",
    " 3rd Qu.:0.15041   3rd Qu.:0.5455   3rd Qu.:0.23409   3rd Qu.:0.3333  \n",
    " Max.   :1.00000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Evaluation With Resampling Methods\n",
    "\n",
    "We can use statistical methods called resampling methods to split our training dataset up into subsets, some are used to train the model and others are held back and used to estimate the accuracy of the model on unseen data.\n",
    "The different resampling methods are available in the caret package. Look up the help on the **createDataPartition()**, **trainControl()** and **train()** functions in R.\n",
    "- Split a dataset into training and test sets.\n",
    "- Estimate the accuracy of an algorithm using k-fold cross validation.\n",
    "- Estimate the accuracy of an algorithm using repeated k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training control\n",
    "trainControl <- trainControl(method=\"cv\", number=10)\n",
    "# estimate the accuracy of Naive Bayes on the dataset\n",
    "fit <- train(diabetes~., data=PimaIndiansDiabetes, trControl=trainControl, method=\"nb\")\n",
    "# summarize the estimated accuracy\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Naive Bayes \n",
    "\n",
    "768 samples\n",
    "  8 predictor\n",
    "  2 classes: 'neg', 'pos' \n",
    "\n",
    "No pre-processing\n",
    "Resampling: Cross-Validated (10 fold) \n",
    "Summary of sample sizes: 691, 692, 691, 691, 691, 691, ... \n",
    "Resampling results across tuning parameters:\n",
    "\n",
    "  usekernel  Accuracy   Kappa    \n",
    "  FALSE      0.7539986  0.4464309\n",
    "   TRUE      0.7566131  0.4489851\n",
    "\n",
    "Tuning parameter 'fL' was held constant at a value of 0\n",
    "Tuning\n",
    " parameter 'adjust' was held constant at a value of 1\n",
    "Accuracy was used to select the optimal model using the largest value.\n",
    "The final values used for the model were fL = 0, usekernel = TRUE and adjust\n",
    " = 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Evaluation Metrics\n",
    "There are many different metrics that you can use to evaluate the skill of a machine learning algorithm on a dataset.\n",
    "\n",
    "You can specify the metric used for your test harness in caret in the **train()** function and defaults can be used for regression and classification problems.\n",
    "\n",
    "- Practice using the Accuracy and Kappa metrics on a classification problem (e.g. iris dataset).\n",
    "- Practice using RMSE and RSquared metrics on a regression problem (e.g. longley dataset).\n",
    "- Practice using the ROC metrics on a binary classification problem (e.g. PimaIndiansDiabetes dataset from the mlbench package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare 5-fold cross validation and keep the class probabilities\n",
    "control <- trainControl(method=\"cv\", number=5, classProbs=TRUE, summaryFunction=mnLogLoss)\n",
    "# estimate accuracy using LogLoss of the CART algorithm\n",
    "fit <- train(diabetes~., data=PimaIndiansDiabetes, method=\"rpart\", metric=\"logLoss\", trControl=control)\n",
    "# display results\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CART \n",
    "\n",
    "768 samples\n",
    "  8 predictor\n",
    "  2 classes: 'neg', 'pos' \n",
    "\n",
    "No pre-processing\n",
    "Resampling: Cross-Validated (5 fold) \n",
    "Summary of sample sizes: 615, 614, 615, 614, 614 \n",
    "Resampling results across tuning parameters:\n",
    "\n",
    "  cp          logLoss  \n",
    "  0.01741294  0.5500441\n",
    "  0.10447761  0.5752074\n",
    "  0.24253731  0.5752882\n",
    "\n",
    "logLoss was used to select the optimal model using the smallest value.\n",
    "The final value used for the model was cp = 0.01741294.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot-Check Algorithms\n",
    "\n",
    "We have to discover which which algorithm will perform best on our data using a process of trial and error. That is called spot-checking algorithms. The caret package provides an interface to many machine learning algorithms and tools to compare the estimated accuracy of those algorithms.\n",
    "- Spot check linear algorithms on a dataset (e.g. linear regression, logistic regression and linear discriminate analysis).\n",
    "- Spot check some non-linear algorithms on a dataset (e.g. KNN, SVM and CART).\n",
    "- Spot-check some sophisticated ensemble algorithms on a dataset (e.g. random forest and stochastic gradient boosting).\n",
    "\n",
    "**Help:** We can get a list of models that we can use in caret by typing: **names(getModelInfo())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare 10-fold cross validation\n",
    "trainControl <- trainControl(method=\"cv\", number=10)\n",
    "# estimate accuracy of logistic regression\n",
    "set.seed(7)\n",
    "fit.lr <- train(diabetes~., data=PimaIndiansDiabetes, method=\"glm\", trControl=trainControl)\n",
    "# estimate accuracy of linear discriminate analysis\n",
    "set.seed(7)\n",
    "fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method=\"lda\", trControl=trainControl)\n",
    "# collect resampling statistics\n",
    "results <- resamples(list(LR=fit.lr, LDA=fit.lda))\n",
    "# summarize results\n",
    "summary(results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Call:\n",
    "summary.resamples(object = results)\n",
    "\n",
    "Models: LR, LDA \n",
    "Number of resamples: 10 \n",
    "\n",
    "Accuracy \n",
    "         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n",
    "LR  0.7272727 0.7435065 0.7711893 0.7839884 0.8019481 0.9078947    0\n",
    "LDA 0.7272727 0.7337662 0.7711893 0.7800923 0.8019481 0.9078947    0\n",
    "\n",
    "Kappa \n",
    "         Min.   1st Qu.    Median      Mean   3rd Qu.    Max. NA's\n",
    "LR  0.3631351 0.4101647 0.4690584 0.5003949 0.5517138 0.78125    0\n",
    "LDA 0.3631351 0.3867983 0.4690584 0.4892890 0.5517138 0.78125    0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison and Selection\n",
    "Now that We know how to spot check machine learning algorithms on our dataset, we need to know how to compare the estimated performance of different algorithms and select the best model.\n",
    "\n",
    "The caret package provides a suite of tools to plot and summarize the differences in performance between models.\n",
    "- Use the summary() caret function to create a table of results.\n",
    "- Use the dotplot() caret function to compare results.\n",
    "- Use the bwplot() caret function to compare results.\n",
    "- Use the diff() caret function to calculate the statistical significance between results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "dotplot(results)\n",
    "bwplot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistical significance\n",
    "diff(results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Call:\n",
    "diff.resamples(x = results)\n",
    "\n",
    "Models: LR, LDA \n",
    "Metrics: Accuracy, Kappa \n",
    "Number of differences: 1 \n",
    "p-value adjustment: bonferroni "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Tuning\n",
    "Once we have found one or two algorithms that perform well on our dataset, we may want to improve the performance of those models.One way to increase the performance of an algorithm is to tune it’s parameters to our specific dataset.\n",
    "\n",
    "The caret package provides three ways to search for combinations of parameters for a machine learning algorithm.\n",
    "\n",
    "- Tune the parameters of an algorithm automatically (e.g. see the tuneLength argument to train()).\n",
    "- Tune the parameters of an algorithm using a grid search that we specify.\n",
    "- Tune the parameters of an algorithm using a random search.\n",
    "\n",
    "Take a look at the help for the **trainControl()** and **train()** functions and take note of the method and the tuneGrid arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "library(caret)\n",
    "# load the iris dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# define training control\n",
    "trainControl <- trainControl(method=\"cv\", number=10)\n",
    "# define a grid of parameters to search for random forest\n",
    "grid <- expand.grid(.mtry=c(1,2,3,4,5,6,7,8,10))\n",
    "# estimate the accuracy of Random Forest on the dataset\n",
    "fit <- train(diabetes~., data=PimaIndiansDiabetes, trControl=trainControl, tuneGrid=grid, method=\"rf\")\n",
    "# summarize the estimated accuracy\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random Forest \n",
    "\n",
    "768 samples\n",
    "  8 predictor\n",
    "  2 classes: 'neg', 'pos' \n",
    "\n",
    "No pre-processing\n",
    "Resampling: Cross-Validated (10 fold) \n",
    "Summary of sample sizes: 691, 691, 691, 691, 692, 691, ... \n",
    "Resampling results across tuning parameters:\n",
    "\n",
    "  mtry  Accuracy   Kappa    \n",
    "   1    0.7511620  0.4236067\n",
    "   2    0.7706425  0.4809448\n",
    "   3    0.7666781  0.4797696\n",
    "   4    0.7602358  0.4604994\n",
    "   5    0.7654648  0.4746157\n",
    "   6    0.7628332  0.4691105\n",
    "   7    0.7628332  0.4656115\n",
    "   8    0.7628674  0.4666438\n",
    "  10    0.7641661  0.4708714\n",
    "\n",
    "Accuracy was used to select the optimal model using the largest value.\n",
    "The final value used for the model was mtry = 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Predictions\n",
    "Another way that we can improve the performance of our models is to combine the predictions from multiple models.\n",
    "\n",
    "Some models provide this capability built-in such as **random forest** for *bagging* and **stochastic gradient boosting** for *boosting*. Another type of ensembling called **stacking (or blending)** can learn how to best combine the predictions from multiple models and is provided in the package *caretEnsemble.*\n",
    "\n",
    "- Bagging ensembles with the random forest and bagged CART algorithms in caret.\n",
    "- Boosting ensembles with the gradient boosting machine and C5.0 algorithms in caret.\n",
    "- Stacking ensembles using the caretEnsemble package and the caretStack() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "library(mlbench)\n",
    "library(caret)\n",
    "library(caretEnsemble)\n",
    "# load the Pima Indians Diabetes dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# create sub-models\n",
    "trainControl <- trainControl(method=\"cv\", number=5, savePredictions=TRUE, classProbs=TRUE)\n",
    "algorithmList <- c('knn', 'glm')\n",
    "set.seed(7)\n",
    "models <- caretList(diabetes~., data=PimaIndiansDiabetes, trControl=trainControl, methodList=algorithmList)\n",
    "print(models)\n",
    "# learn how to best combine the predictions\n",
    "stackControl <- trainControl(method=\"cv\", number=5, savePredictions=TRUE, classProbs=TRUE)\n",
    "set.seed(7)\n",
    "stack.glm <- caretStack(models, method=\"glm\", trControl=stackControl)\n",
    "print(stack.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize And Save Model\n",
    "The tasks related to finalizing our model.\n",
    "\n",
    "- Using the predict() function to make predictions with a model trained using caret.\n",
    "- Training standalone versions of well performing models.\n",
    "- Saving trained models to file and loading them up again using the saveRDS() and readRDS() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load package\n",
    "library(randomForest)\n",
    "# load iris data\n",
    "data(PimaIndiansDiabetes)\n",
    "# train random forest model\n",
    "finalModel <- randomForest(diabetes~., PimaIndiansDiabetes, mtry=2, ntree=2000)\n",
    "# display the details of the final model\n",
    "print(finalModel)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Call:\n",
    " randomForest(formula = diabetes ~ ., data = PimaIndiansDiabetes,      mtry = 2, ntree = 2000) \n",
    "               Type of random forest: classification\n",
    "                     Number of trees: 2000\n",
    "No. of variables tried at each split: 2\n",
    "\n",
    "        OOB estimate of  error rate: 23.05%\n",
    "Confusion matrix:\n",
    "    neg pos class.error\n",
    "neg 427  73   0.1460000\n",
    "pos 104 164   0.3880597"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
