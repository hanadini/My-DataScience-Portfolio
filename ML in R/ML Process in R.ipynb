{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare problem\n",
    "- Load libraries\n",
    "- Load Dataset\n",
    "- Split-out validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"mlbench\")\n",
    "library(mlbench)\n",
    "data(package=\"mlbench\")\n",
    "data(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Data\n",
    "\n",
    "### Understand Data with Descriptive Statistics\n",
    "\n",
    "- Understand your data using the **head()** function to look at the first few rows.\n",
    "- Review the distribution of your data with the **summary()** function.\n",
    "- Review the dimensions of your data with the **dim()** function.\n",
    "- Review the types of the attributes in your data with **sapply()**\n",
    "- Class Distribution, know the proportion of instances that belong to each class value.**cbind()**\n",
    "- The standard deviation along with the mean are useful to know if the data has a Gaussian (or nearly Gaussian) distribution.**sapply()**\n",
    "- Skewness, If a distribution looks kind-of-Gaussian but is pushed far left or right it is useful to know the skew.**skewness**\n",
    "- Calculate pair-wise correlation between your variables using the **cor()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n",
    "1        6     148       72      35       0 33.6    0.627  50      pos\n",
    "2        1      85       66      29       0 26.6    0.351  31      neg\n",
    "3        8     183       64       0       0 23.3    0.672  32      pos\n",
    "4        1      89       66      23      94 28.1    0.167  21      neg\n",
    "5        0     137       40      35     168 43.1    2.288  33      pos\n",
    "6        5     116       74       0       0 25.6    0.201  30      neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "   pregnant         glucose         pressure         triceps     \n",
    " Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n",
    " 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n",
    " Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n",
    " Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n",
    " 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n",
    " Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n",
    "    insulin           mass          pedigree           age        diabetes \n",
    " Min.   :  0.0   Min.   : 0.00   Min.   :0.0780   Min.   :21.00   neg:500  \n",
    " 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437   1st Qu.:24.00   pos:268  \n",
    " Median : 30.5   Median :32.00   Median :0.3725   Median :29.00            \n",
    " Mean   : 79.8   Mean   :31.99   Mean   :0.4719   Mean   :33.24            \n",
    " 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262   3rd Qu.:41.00            \n",
    " Max.   :846.0   Max.   :67.10   Max.   :2.4200   Max.   :81.00 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(PimaIndiansDiabetes)\n",
    "[1] 768   9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "library(mlbench)\n",
    "# load dataset\n",
    "data(BostonHousing)\n",
    "# list types for each attribute\n",
    "sapply(BostonHousing, class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "library(mlbench)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# distribution of class variable\n",
    "y <- PimaIndiansDiabetes$diabetes\n",
    "cbind(freq=table(y), percentage=prop.table(table(y))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "library(mlbench)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# calculate standard deviation for all attributes\n",
    "sapply(PimaIndiansDiabetes[,1:8], sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(mlbench)\n",
    "library(e1071)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# calculate skewness for each variable\n",
    "skew <- apply(PimaIndiansDiabetes[,1:8], 2, skewness)\n",
    "# display skewness, larger/smaller deviations from 0 show more skew\n",
    "print(skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "library(mlbench)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# calculate a correlation matrix for numeric variables\n",
    "correlations <- cor(PimaIndiansDiabetes[,1:8])\n",
    "# display the correlation matrix\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Data with Visualization\n",
    "\n",
    "- Use the **hist()** function to create a histogram of each attribute.\n",
    "- Use the **boxplot()** function to create box and whisker plots of each attribute.\n",
    "- Use the **pairs()** function to create pair-wise scatterplots of all attributes.\n",
    "    - Use the **plot=\"pairs\"** Scatterplot Matrix shows a grid of scatterplots where each attribute is plotted against all      other attributes.\n",
    "- Use the **plot=\"density\"** Density estimation plots (density plots for short) summarize the distribution of the data.\n",
    "- Use the **plot=\"box\"** Box and Whisker plots (or box plots for short) summarize the distribution of a given attribute by showing a box for the 25th and 75th percentile, a line in the box for the 50th percentile (median) and a dot for the mean. The whiskers show 1.5*the height of the box (called the Inter Quartile Range) which indicate the expected range of the data and any data beyond those whiskers is assumed to be an outlier and marked with a dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "library(caret)\n",
    "# load the data\n",
    "data(iris)\n",
    "# pair-wise plots of all 4 attributes, dots colored by class\n",
    "featurePlot(x=iris[,1:4], y=iris[,5], plot=\"pairs\", auto.key=list(columns=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "library(caret)\n",
    "# load the data\n",
    "data(iris)\n",
    "# density plots for each attribute by class value\n",
    "featurePlot(x=iris[,1:4], y=iris[,5], plot=\"density\", scales=list(x=list(relation=\"free\"), y=list(relation=\"free\")), auto.key=list(columns=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "library(caret)\n",
    "# load the data\n",
    "data(iris)\n",
    "# box and whisker plots for each attribute by class value\n",
    "featurePlot(x=iris[,1:4], y=iris[,5], plot=\"box\", scales=list(x=list(relation=\"free\"), y=list(relation=\"free\")), auto.key=list(columns=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare For Modeling by Pre-Processing Data\n",
    "\n",
    "- **Normalize:** Data values can be scaled into the range of [0, 1] which is called normalization. *method=c(\"range\")* \n",
    "\n",
    "- **Scale:** The scale transform calculates the standard deviation for an attribute and divides each value by that standard deviation.*method=c(\"scale\")*\n",
    "\n",
    "- **Center:** The center transform calculates the mean for an attribute and subtracts it from each value. *method=c(\"center\")*\n",
    "\n",
    "- **Standardize:** Combining the scale and center transforms will standardize your data. Attributes will have a mean value of 0 and a standard deviation of 1. *method=c(\"center\", \"scale\")*\n",
    "\n",
    "- **Box-Cox Transform:** When an attribute has a Gaussian-like distribution but is shifted, this is called a skew. The distribution of an attribute can be shifted to reduce the skew and make it more Gaussian. The BoxCox transform can perform this operation (assumes all values are positive, greater than zero).ion. *method=c(\"BoxCox\")*\n",
    "\n",
    "- **Yeo-Johnson Transform:** Another power-transform like the Box-Cox transform, but it supports raw values that are equal to zero and negative. *method=c(\"YeoJohnson\")*\n",
    "\n",
    "- **Principal Component Analysis:** Transform the data to the principal components. The transform keeps components above the variance threshold (default=0.95) or the number of components can be specified (pcaComp). The result is attributes that are uncorrelated, useful for algorithms like linear and generalized linear regression. *method=c(\"center\", \"scale\", \"pca\")*\n",
    " \n",
    "- **Independent Component Analysis:** Transform the data to the independent components. Unlike PCA, ICA retains those components that are independent. You must specify the number of desired independent components with the n.comp argument. Useful for algorithms such as naive bayes. *method=c(\"center\", \"scale\", \"ica\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize: scaled into the range of [0, 1]\n",
    "install.packages(\"caret\")\n",
    "# load caret package\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(PimaIndiansDiabetes[,1:8], method=c(\"range\"))\n",
    "# transform the dataset using the pre-processing parameters\n",
    "transformed <- predict(preprocessParams, PimaIndiansDiabetes[,1:8])\n",
    "# summarize the transformed dataset\n",
    "summary(transformed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pregnant          glucose          pressure         triceps      \n",
    " Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n",
    " 1st Qu.:0.05882   1st Qu.:0.4975   1st Qu.:0.5082   1st Qu.:0.0000  \n",
    " Median :0.17647   Median :0.5879   Median :0.5902   Median :0.2323  \n",
    " Mean   :0.22618   Mean   :0.6075   Mean   :0.5664   Mean   :0.2074  \n",
    " 3rd Qu.:0.35294   3rd Qu.:0.7048   3rd Qu.:0.6557   3rd Qu.:0.3232  \n",
    " Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n",
    "    insulin             mass           pedigree            age        \n",
    " Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n",
    " 1st Qu.:0.00000   1st Qu.:0.4069   1st Qu.:0.07077   1st Qu.:0.0500  \n",
    " Median :0.03605   Median :0.4769   Median :0.12575   Median :0.1333  \n",
    " Mean   :0.09433   Mean   :0.4768   Mean   :0.16818   Mean   :0.2040  \n",
    " 3rd Qu.:0.15041   3rd Qu.:0.5455   3rd Qu.:0.23409   3rd Qu.:0.3333  \n",
    " Max.   :1.00000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale:\n",
    "# load libraries\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# summarize data\n",
    "summary(iris[,1:4])\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(iris[,1:4], method=c(\"scale\"))\n",
    "# summarize transform parameters\n",
    "print(preprocessParams)\n",
    "# transform the dataset using the parameters\n",
    "transformed <- predict(preprocessParams, iris[,1:4])\n",
    "# summarize the transformed dataset\n",
    "summary(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center:\n",
    "# load libraries\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# summarize data\n",
    "summary(iris[,1:4])\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(iris[,1:4], method=c(\"center\"))\n",
    "# summarize transform parameters\n",
    "print(preprocessParams)\n",
    "# transform the dataset using the parameters\n",
    "transformed <- predict(preprocessParams, iris[,1:4])\n",
    "# summarize the transformed dataset\n",
    "summary(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize:\n",
    "# load libraries\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# summarize data\n",
    "summary(iris[,1:4])\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(iris[,1:4], method=c(\"center\", \"scale\"))\n",
    "# summarize transform parameters\n",
    "print(preprocessParams)\n",
    "# transform the dataset using the parameters\n",
    "transformed <- predict(preprocessParams, iris[,1:4])\n",
    "# summarize the transformed dataset\n",
    "summary(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox\n",
    "# load libraries\n",
    "library(mlbench)\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# summarize pedigree and age\n",
    "summary(PimaIndiansDiabetes[,7:8])\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c(\"BoxCox\"))\n",
    "# summarize transform parameters\n",
    "print(preprocessParams)\n",
    "# transform the dataset using the parameters\n",
    "transformed <- predict(preprocessParams, PimaIndiansDiabetes[,7:8])\n",
    "# summarize the transformed dataset (note pedigree and age)\n",
    "summary(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeo-Johnson Transform\n",
    "# load libraries\n",
    "library(mlbench)\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# summarize pedigree and age\n",
    "summary(PimaIndiansDiabetes[,7:8])\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c(\"YeoJohnson\"))\n",
    "# summarize transform parameters\n",
    "print(preprocessParams)\n",
    "# transform the dataset using the parameters\n",
    "transformed <- predict(preprocessParams, PimaIndiansDiabetes[,7:8])\n",
    "# summarize the transformed dataset (note pedigree and age)\n",
    "summary(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "# load the libraries\n",
    "library(mlbench)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# summarize dataset\n",
    "summary(iris)\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(iris, method=c(\"center\", \"scale\", \"pca\"))\n",
    "# summarize transform parameters\n",
    "print(preprocessParams)\n",
    "# transform the dataset using the parameters\n",
    "transformed <- predict(preprocessParams, iris)\n",
    "# summarize the transformed dataset\n",
    "summary(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independent Component Analysis\n",
    "# load libraries\n",
    "library(mlbench)\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# summarize dataset\n",
    "summary(PimaIndiansDiabetes[,1:8])\n",
    "# calculate the pre-process parameters from the dataset\n",
    "preprocessParams <- preProcess(PimaIndiansDiabetes[,1:8], method=c(\"center\", \"scale\", \"ica\"), n.comp=5)\n",
    "# summarize transform parameters\n",
    "print(preprocessParams)\n",
    "# transform the dataset using the parameters\n",
    "transformed <- predict(preprocessParams, PimaIndiansDiabetes[,1:8])\n",
    "# summarize the transformed dataset\n",
    "summary(transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Evaluation With Resampling Methods\n",
    "\n",
    "We can use statistical methods called resampling methods to split our training dataset up into subsets, some are used to train the model and others are held back and used to estimate the accuracy of the model on unseen data.\n",
    "The different resampling methods are available in the caret package. Look up the help on the **createDataPartition()**, **trainControl()** and **train()** functions in R.\n",
    "\n",
    "- **Data Split:** Data splitting involves partitioning the data into an explicit training dataset used to prepare the model and an unseen test dataset used to evaluate the models performance on unseen data.\n",
    "\n",
    "- **Bootstrap:** Bootstrap resampling involves taking random samples from the dataset (with re-selection) against which to evaluate the model. In aggregate, the results provide an indication of the variance of the models performance. Typically, large number of resampling iterations are performed (thousands or tends of thousands).\n",
    "\n",
    "- **k-fold Cross Validation:** The k-fold cross validation method involves splitting the dataset into k-subsets. For each subset is held out while the model is trained on all other subsets. This process is completed until accuracy is determine for each instance in the dataset, and an overall accuracy estimate is provided.\n",
    "\n",
    "It is a robust method for estimating accuracy, and the size of k and tune the amount of bias in the estimate, with popular values set to 3, 5, 7 and 10.\n",
    "\n",
    "- **Repeated k-fold Cross Validation:** The process of splitting the data into k-folds can be repeated a number of times, this is called Repeated k-fold Cross Validation. The final model accuracy is taken as the mean from the number of repeats.\n",
    "\n",
    "- **Leave One Out Cross Validation:** In Leave One Out Cross Validation (LOOCV), a data instance is left out and a model constructed on all other data instances in the training set. This is repeated for all data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training control\n",
    "trainControl <- trainControl(method=\"cv\", number=10)\n",
    "# estimate the accuracy of Naive Bayes on the dataset\n",
    "fit <- train(diabetes~., data=PimaIndiansDiabetes, trControl=trainControl, method=\"nb\")\n",
    "# summarize the estimated accuracy\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Naive Bayes \n",
    "\n",
    "768 samples\n",
    "  8 predictor\n",
    "  2 classes: 'neg', 'pos' \n",
    "\n",
    "No pre-processing\n",
    "Resampling: Cross-Validated (10 fold) \n",
    "Summary of sample sizes: 691, 692, 691, 691, 691, 691, ... \n",
    "Resampling results across tuning parameters:\n",
    "\n",
    "  usekernel  Accuracy   Kappa    \n",
    "  FALSE      0.7539986  0.4464309\n",
    "   TRUE      0.7566131  0.4489851\n",
    "\n",
    "Tuning parameter 'fL' was held constant at a value of 0\n",
    "Tuning\n",
    " parameter 'adjust' was held constant at a value of 1\n",
    "Accuracy was used to select the optimal model using the largest value.\n",
    "The final values used for the model were fL = 0, usekernel = TRUE and adjust\n",
    " = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "# load the libraries\n",
    "library(caret)\n",
    "library(klaR)\n",
    "# load the iris dataset\n",
    "data(iris)\n",
    "# define an 80%/20% train/test split of the dataset\n",
    "split=0.80\n",
    "trainIndex <- createDataPartition(iris$Species, p=split, list=FALSE)\n",
    "data_train <- iris[ trainIndex,]\n",
    "data_test <- iris[-trainIndex,]\n",
    "# train a naive bayes model\n",
    "model <- NaiveBayes(Species~., data=data_train)\n",
    "# make predictions\n",
    "x_test <- data_test[,1:4]\n",
    "y_test <- data_test[,5]\n",
    "predictions <- predict(model, x_test)\n",
    "# summarize results\n",
    "confusionMatrix(predictions$class, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the iris dataset\n",
    "data(iris)\n",
    "# define training control\n",
    "train_control <- trainControl(method=\"boot\", number=100)\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, trControl=train_control, method=\"nb\")\n",
    "# summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold Cross Validation\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the iris dataset\n",
    "data(iris)\n",
    "# define training control\n",
    "train_control <- trainControl(method=\"cv\", number=10)\n",
    "# fix the parameters of the algorithm\n",
    "grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, trControl=train_control, method=\"nb\", tuneGrid=grid)\n",
    "# summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeated k-fold Cross Validation\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the iris dataset\n",
    "data(iris)\n",
    "# define training control\n",
    "train_control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, trControl=train_control, method=\"nb\")\n",
    "# summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave One Out Cross Validation\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the iris dataset\n",
    "data(iris)\n",
    "# define training control\n",
    "train_control <- trainControl(method=\"LOOCV\")\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, trControl=train_control, method=\"nb\")\n",
    "# summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Evaluation Metrics\n",
    "There are many different metrics that you can use to evaluate the skill of a machine learning algorithm on a dataset.\n",
    "\n",
    "You can specify the metric used for your test harness in caret in the **train()** function and defaults can be used for regression and classification problems.\n",
    "\n",
    "- **Accuracy and Kappa:** These are the default metrics used to evaluate algorithms on binary and multi-class classification datasets in caret.\n",
    "\n",
    "*Accuracy* is the percentage of correctly classifies instances out of all instances. It is more useful on a binary classification than multi-class classification problems because it can be less clear exactly how the accuracy breaks down across those classes (e.g. you need to go deeper with a confusion matrix).\n",
    "\n",
    "*Kappa* or Cohen’s Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on dataset. It is a more useful measure to use on problems that have an imbalance in the classes (e.g. 70-30 split for classes 0 and 1 and you can achieve 70% accuracy by predicting all instances are for class 0).\n",
    "\n",
    "- **RMSE and R^2:** These are the default metrics used to evaluate algorithms on regression datasets in caret.\n",
    "\n",
    "*RMSE or Root Mean Squared Error* is the average deviation of the predictions from the observations. It is useful to get a gross idea of how well (or not) an algorithm is doing, in the units of the output variable.\n",
    "\n",
    "*R^2 spoken as R Squared* or also called the coefficient of determination provides a “goodness of fit” measure for the predictions to the observations. This is a value between 0 and 1 for no-fit and perfect fit respectively.\n",
    "\n",
    "- **ROC (AUC, Sensitivity and Specificity):** ROC metrics are only suitable for binary classification problems (e.g. two classes).\n",
    "\n",
    "To calculate ROC information, you must change the summaryFunction in your trainControl to be twoClassSummary. This will calculate the Area Under ROC Curve (AUROC) also called just Area Under curve (AUC), sensitivity and specificity.\n",
    "\n",
    "ROC is actually the area under the ROC curve or AUC. The AUC represents a models ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predicts perfectly. An area of 0.5 represents a model as good as random.\n",
    "\n",
    "ROC can be broken down into sensitivity and specificity. A binary classification problem is really a trade-off between sensitivity and specificity.\n",
    "\n",
    "Sensitivity is the true positive rate also called the recall. It is the number instances from the positive (first) class that actually predicted correctly.\n",
    "\n",
    "Specificity is also called the true negative rate. Is the number of instances from the negative class (second) class that were actually predicted correctly.\n",
    "\n",
    "- **LogLoss:** Logarithmic Loss or LogLoss is used to evaluate binary classification but it is more common for multi-class classification algorithms. Specifically, it evaluates the probabilities estimated by the algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare 5-fold cross validation and keep the class probabilities\n",
    "control <- trainControl(method=\"cv\", number=5, classProbs=TRUE, summaryFunction=mnLogLoss)\n",
    "# estimate accuracy using LogLoss of the CART algorithm\n",
    "fit <- train(diabetes~., data=PimaIndiansDiabetes, method=\"rpart\", metric=\"logLoss\", trControl=control)\n",
    "# display results\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CART \n",
    "\n",
    "768 samples\n",
    "  8 predictor\n",
    "  2 classes: 'neg', 'pos' \n",
    "\n",
    "No pre-processing\n",
    "Resampling: Cross-Validated (5 fold) \n",
    "Summary of sample sizes: 615, 614, 615, 614, 614 \n",
    "Resampling results across tuning parameters:\n",
    "\n",
    "  cp          logLoss  \n",
    "  0.01741294  0.5500441\n",
    "  0.10447761  0.5752074\n",
    "  0.24253731  0.5752882\n",
    "\n",
    "logLoss was used to select the optimal model using the smallest value.\n",
    "The final value used for the model was cp = 0.01741294.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy and Kappa\n",
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# prepare resampling method\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "set.seed(7)\n",
    "fit <- train(diabetes~., data=PimaIndiansDiabetes, method=\"glm\", metric=\"Accuracy\", trControl=control)\n",
    "# display results\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE and R^2\n",
    "# load libraries\n",
    "library(caret)\n",
    "# load data\n",
    "data(longley)\n",
    "# prepare resampling method\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "set.seed(7)\n",
    "fit <- train(Employed~., data=longley, method=\"lm\", metric=\"RMSE\", trControl=control)\n",
    "# display results\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area Under ROC Curve\n",
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# prepare resampling method\n",
    "control <- trainControl(method=\"cv\", number=5, classProbs=TRUE, summaryFunction=twoClassSummary)\n",
    "set.seed(7)\n",
    "fit <- train(diabetes~., data=PimaIndiansDiabetes, method=\"glm\", metric=\"ROC\", trControl=control)\n",
    "# display results\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logarithmic Loss\n",
    "# load libraries\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# prepare resampling method\n",
    "control <- trainControl(method=\"cv\", number=5, classProbs=TRUE, summaryFunction=mnLogLoss)\n",
    "set.seed(7)\n",
    "fit <- train(Species~., data=iris, method=\"rpart\", metric=\"logLoss\", trControl=control)\n",
    "# display results\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot-Check Algorithms\n",
    "\n",
    "We have to discover which which algorithm will perform best on our data using a process of trial and error. That is called spot-checking algorithms. The caret package provides an interface to many machine learning algorithms and tools to compare the estimated accuracy of those algorithms.\n",
    "\n",
    "Algorithms are presented in two groups:\n",
    "  - *Linear Algorithms that are simpler methods that have a strong bias but are fast to train.*(e.g. linear regression, logistic regression and linear discriminate analysis)\n",
    "  - *Nonlinear Algorithms that are more complex methods that have a large variance but are often more accurate.* (e.g. KNN, SVM and CART)\n",
    " \n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Linear Discriminant Analysis\n",
    "- Regularized Regression\n",
    "- k-Nearest Neighbors\n",
    "- Naive Bayes\n",
    "- Support Vector Machine\n",
    "- Classification and Regression Trees\n",
    "\n",
    "\n",
    " Spot-check some sophisticated ensemble algorithms on a dataset (e.g. random forest and stochastic gradient boosting).\n",
    "\n",
    "**Help:** We can get a list of models that we can use in caret by typing: **names(getModelInfo())**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algorithms\n",
    "These are methods that make large assumptions about the form of the function being modeled. As such they are have a high bias but are often fast to train.\n",
    "\n",
    "The final models are also often easy (or easier) to interpret, making them desirable as final models. If the results are suitably accurate, you may not need to move onto non-linear methods if a linear algorithm.\n",
    "\n",
    "1. Linear Regression\n",
    "The **lm()** function is in the stats library and creates a linear regression model using ordinary least squares.\n",
    "2. Logistic Regression\n",
    "The **glm** function is in the stats library and creates a generalized linear model. It can be configured to perform a logistic regression suitable for binary classification problems.\n",
    "3. Linear Discriminant Analysis\n",
    "The **lda** function is in the MASS library and creates a linear model of a classification problem.\n",
    "4. Regularized Regression\n",
    "The **glmnet** function is in the glmnet library and can be used for classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare 10-fold cross validation\n",
    "trainControl <- trainControl(method=\"cv\", number=10)\n",
    "# estimate accuracy of logistic regression\n",
    "set.seed(7)\n",
    "fit.lr <- train(diabetes~., data=PimaIndiansDiabetes, method=\"glm\", trControl=trainControl)\n",
    "# estimate accuracy of linear discriminate analysis\n",
    "set.seed(7)\n",
    "fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method=\"lda\", trControl=trainControl)\n",
    "# collect resampling statistics\n",
    "results <- resamples(list(LR=fit.lr, LDA=fit.lda))\n",
    "# summarize results\n",
    "summary(results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Call:\n",
    "summary.resamples(object = results)\n",
    "\n",
    "Models: LR, LDA \n",
    "Number of resamples: 10 \n",
    "\n",
    "Accuracy \n",
    "         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n",
    "LR  0.7272727 0.7435065 0.7711893 0.7839884 0.8019481 0.9078947    0\n",
    "LDA 0.7272727 0.7337662 0.7711893 0.7800923 0.8019481 0.9078947    0\n",
    "\n",
    "Kappa \n",
    "         Min.   1st Qu.    Median      Mean   3rd Qu.    Max. NA's\n",
    "LR  0.3631351 0.4101647 0.4690584 0.5003949 0.5517138 0.78125    0\n",
    "LDA 0.3631351 0.3867983 0.4690584 0.4892890 0.5517138 0.78125    0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "library(mlbench)\n",
    "# load data\n",
    "data(BostonHousing)\n",
    "# fit model\n",
    "fit <- lm(medv~., BostonHousing)\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, BostonHousing)\n",
    "# summarize accuracy\n",
    "mse <- mean((BostonHousing$medv - predictions)^2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# load dataset\n",
    "data(BostonHousing)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.lm <- train(medv~., data=BostonHousing, method=\"lm\", metric=\"RMSE\", preProc=c(\"center\", \"scale\"), trControl=control)\n",
    "# summarize fit\n",
    "print(fit.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# fit model\n",
    "fit <- glm(diabetes~., data=PimaIndiansDiabetes, family=binomial(link='logit'))\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "probabilities <- predict(fit, PimaIndiansDiabetes[,1:8], type='response')\n",
    "predictions <- ifelse(probabilities > 0.5,'pos','neg')\n",
    "# summarize accuracy\n",
    "table(predictions, PimaIndiansDiabetes$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method=\"glm\", metric=\"Accuracy\", preProc=c(\"center\", \"scale\"), trControl=control)\n",
    "# summarize fit\n",
    "print(fit.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "library(MASS)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# fit model\n",
    "fit <- lda(diabetes~., data=PimaIndiansDiabetes)\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, PimaIndiansDiabetes[,1:8])$class\n",
    "# summarize accuracy\n",
    "table(predictions, PimaIndiansDiabetes$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method=\"lda\", metric=\"Accuracy\", preProc=c(\"center\", \"scale\"), trControl=control)\n",
    "# summarize fit\n",
    "print(fit.lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularized Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Example\n",
    "# load the library\n",
    "library(glmnet)\n",
    "library(mlbench)\n",
    "# load data\n",
    "data(PimaIndiansDiabetes)\n",
    "x <- as.matrix(PimaIndiansDiabetes[,1:8])\n",
    "y <- as.matrix(PimaIndiansDiabetes[,9])\n",
    "# fit model\n",
    "fit <- glmnet(x, y, family=\"binomial\", alpha=0.5, lambda=0.001)\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, x, type=\"class\")\n",
    "# summarize accuracy\n",
    "table(predictions, PimaIndiansDiabetes$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Example\n",
    "# load the libraries\n",
    "library(glmnet)\n",
    "library(mlbench)\n",
    "# load data\n",
    "data(BostonHousing)\n",
    "BostonHousing$chas <- as.numeric(as.character(BostonHousing$chas))\n",
    "x <- as.matrix(BostonHousing[,1:13])\n",
    "y <- as.matrix(BostonHousing[,14])\n",
    "# fit model\n",
    "fit <- glmnet(x, y, family=\"gaussian\", alpha=0.5, lambda=0.001)\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, x, type=\"link\")\n",
    "# summarize accuracy\n",
    "mse <- mean((y - predictions)^2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "library(glmnet)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.glmnet <- train(diabetes~., data=PimaIndiansDiabetes, method=\"glmnet\", metric=\"Accuracy\", preProc=c(\"center\", \"scale\"), trControl=control)\n",
    "# summarize fit\n",
    "print(fit.glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "library(glmnet)\n",
    "# Load the dataset\n",
    "data(BostonHousing)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.glmnet <- train(medv~., data=BostonHousing, method=\"glmnet\", metric=\"RMSE\", preProc=c(\"center\", \"scale\"), trControl=control)\n",
    "# summarize fit\n",
    "print(fit.glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear Algorithms\n",
    "\n",
    "These are machine learning algorithms that make fewer assumptions about the function being modeled. As such, they have a higher variance but are often result in higher accuracy. They increased flexibility also can make them slower to train or increase their memory requirements.\n",
    "\n",
    "1. k-Nearest Neighbors\n",
    "The **knn3** function is in the caret library and does not create a model, rather makes predictions from the training set directly. It can be used for classification or regression.\n",
    "\n",
    "2. Naive Bayes\n",
    "The **naiveBayes** function is in the e1071 library and models the probabilistic of each attribute to the outcome variable independently. It can be used for classification problems.\n",
    "\n",
    "3. Support Vector Machine\n",
    "The **ksvm** function is in the kernlab package and can be used for classification or regression. It is a wrapper for the LIBSVM library and provides a suite of kernel types and configuration options.\n",
    "\n",
    "4. Classification and Regression Trees\n",
    "The **rpart** function in the rpart library provides an implementation of CART for classification and regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "## knn direct classification\n",
    "\n",
    "# load the libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# fit model\n",
    "fit <- knn3(diabetes~., data=PimaIndiansDiabetes, k=3)\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, PimaIndiansDiabetes[,1:8], type=\"class\")\n",
    "# summarize accuracy\n",
    "table(predictions, PimaIndiansDiabetes$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "# load the libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# load data\n",
    "data(BostonHousing)\n",
    "BostonHousing$chas <- as.numeric(as.character(BostonHousing$chas))\n",
    "x <- as.matrix(BostonHousing[,1:13])\n",
    "y <- as.matrix(BostonHousing[,14])\n",
    "# fit model\n",
    "fit <- knnreg(x, y, k=3)\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, x)\n",
    "# summarize accuracy\n",
    "mse <- mean((BostonHousing$medv - predictions)^2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Classification\n",
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method=\"knn\", metric=\"Accuracy\", preProc=c(\"center\", \"scale\"), trControl=control)\n",
    "# summarize fit\n",
    "print(fit.knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "# load libraries\n",
    "library(caret)\n",
    "data(BostonHousing)\n",
    "# Load the dataset\n",
    "data(BostonHousing)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.knn <- train(medv~., data=BostonHousing, method=\"knn\", metric=\"RMSE\", preProc=c(\"center\", \"scale\"), trControl=control)\n",
    "# summarize fit\n",
    "print(fit.knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "library(e1071)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# fit model\n",
    "fit <- naiveBayes(diabetes~., data=PimaIndiansDiabetes)\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, PimaIndiansDiabetes[,1:8])\n",
    "# summarize accuracy\n",
    "table(predictions, PimaIndiansDiabetes$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.nb <- train(diabetes~., data=PimaIndiansDiabetes, method=\"nb\", metric=\"Accuracy\", trControl=control)\n",
    "# summarize fit\n",
    "print(fit.nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    " load the libraries\n",
    "library(kernlab)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# fit model\n",
    "fit <- ksvm(diabetes~., data=PimaIndiansDiabetes, kernel=\"rbfdot\")\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, PimaIndiansDiabetes[,1:8], type=\"response\")\n",
    "# summarize accuracy\n",
    "table(predictions, PimaIndiansDiabetes$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "# load the libraries\n",
    "library(kernlab)\n",
    "library(mlbench)\n",
    "# load data\n",
    "data(BostonHousing)\n",
    "# fit model\n",
    "fit <- ksvm(medv~., BostonHousing, kernel=\"rbfdot\")\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, BostonHousing)\n",
    "# summarize accuracy\n",
    "mse <- mean((BostonHousing$medv - predictions)^2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.svmRadial <- train(diabetes~., data=PimaIndiansDiabetes, method=\"svmRadial\", metric=\"Accuracy\", trControl=control)\n",
    "# summarize fit\n",
    "print(fit.svmRadial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(BostonHousing)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.svmRadial <- train(medv~., data=BostonHousing, method=\"svmRadial\", metric=\"RMSE\", trControl=control)\n",
    "# summarize fit\n",
    "print(fit.svmRadial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "# load the libraries\n",
    "library(rpart)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# fit model\n",
    "fit <- rpart(diabetes~., data=PimaIndiansDiabetes)\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, PimaIndiansDiabetes[,1:8], type=\"class\")\n",
    "# summarize accuracy\n",
    "table(predictions, PimaIndiansDiabetes$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "# load the libraries\n",
    "library(rpart)\n",
    "library(mlbench)\n",
    "# load data\n",
    "data(BostonHousing)\n",
    "# fit model\n",
    "fit <- rpart(medv~., data=BostonHousing, control=rpart.control(minsplit=5))\n",
    "# summarize the fit\n",
    "print(fit)\n",
    "# make predictions\n",
    "predictions <- predict(fit, BostonHousing[,1:13])\n",
    "# summarize accuracy\n",
    "mse <- mean((BostonHousing$medv - predictions)^2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=5)\n",
    "fit.rpart <- train(diabetes~., data=PimaIndiansDiabetes, method=\"rpart\", metric=\"Accuracy\", trControl=control)\n",
    "# summarize fit\n",
    "print(fit.rpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# Load the dataset\n",
    "data(BostonHousing)\n",
    "# train\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"cv\", number=2)\n",
    "fit.rpart <- train(medv~., data=BostonHousing, method=\"rpart\", metric=\"RMSE\", trControl=control)\n",
    "# summarize fit\n",
    "print(fit.rpart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison and Selection\n",
    "Now that We know how to spot check machine learning algorithms on our dataset, we need to know how to compare the estimated performance of different algorithms and select the best model.\n",
    "\n",
    "The caret package provides a suite of tools to plot and summarize the differences in performance between models.\n",
    "- Use the summary() caret function to create a table of results.\n",
    "- Use the bwplot() caret function to compare results.\n",
    "- Use the densityplots() to evaluate the overlap in the estimated behavior of algorithms.\n",
    "- Use the dotplot() caret function to compare results.\n",
    "- Use the parallelplot() to show how each trial of each cross validation fold behaved for each of the algorithms tested.\n",
    "- Use the splom() to compare the same fold-trial results for all other algorithms. All pairs are compared.\n",
    "- Use the xyplot() to compare pair-wise of the accuracy of trial-folds for two machine learning algorithms with an xyplot.\n",
    "- Use the diff() caret function to calculate the statistical significance between results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "dotplot(results)\n",
    "bwplot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistical significance\n",
    "diff(results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Call:\n",
    "diff.resamples(x = results)\n",
    "\n",
    "Models: LR, LDA \n",
    "Metrics: Accuracy, Kappa \n",
    "Number of differences: 1 \n",
    "p-value adjustment: bonferroni "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "library(mlbench)\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# prepare training scheme\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "# train the LVQ model\n",
    "set.seed(7)\n",
    "modelLvq <- train(diabetes~., data=PimaIndiansDiabetes, method=\"lvq\", trControl=control)\n",
    "# train the GBM model\n",
    "set.seed(7)\n",
    "modelGbm <- train(diabetes~., data=PimaIndiansDiabetes, method=\"gbm\", trControl=control, verbose=FALSE)\n",
    "# train the SVM model\n",
    "set.seed(7)\n",
    "modelSvm <- train(diabetes~., data=PimaIndiansDiabetes, method=\"svmRadial\", trControl=control)\n",
    "# collect resamples\n",
    "results <- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm))\n",
    "# summarize the distributions\n",
    "summary(results)\n",
    "# boxplots of results\n",
    "bwplot(results)\n",
    "# dot plots of results\n",
    "dotplot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training scheme\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "# CART\n",
    "set.seed(7)\n",
    "fit.cart <- train(diabetes~., data=PimaIndiansDiabetes, method=\"rpart\", trControl=control)\n",
    "# LDA\n",
    "set.seed(7)\n",
    "fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method=\"lda\", trControl=control)\n",
    "# SVM\n",
    "set.seed(7)\n",
    "fit.svm <- train(diabetes~., data=PimaIndiansDiabetes, method=\"svmRadial\", trControl=control)\n",
    "# kNN\n",
    "set.seed(7)\n",
    "fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method=\"knn\", trControl=control)\n",
    "# Random Forest\n",
    "set.seed(7)\n",
    "fit.rf <- train(diabetes~., data=PimaIndiansDiabetes, method=\"rf\", trControl=control)\n",
    "# collect resamples\n",
    "results <- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Table Summary**\n",
    "\n",
    "This is the easiest comparison that you can do, simply call the summary function() and pass it the resamples result. It will create a table with one algorithm for each row and evaluation metrics for each column. In this case we have sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize differences between modes\n",
    "summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Box and Whisker Plots**\n",
    "\n",
    "This is a useful way to look at the spread of the estimated accuracies for different methods and how they relate.\n",
    "\n",
    "Note that the boxes are ordered from highest to lowest mean accuracy. I find it useful to look at the mean values (dots) and the overlaps of the boxes (middle 50% of results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box and whisker plots to compare models\n",
    "scales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\n",
    "bwplot(results, scales=scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Density Plots**\n",
    "\n",
    "You can show the distribution of model accuracy as density plots. This is a useful way to evaluate the overlap in the estimated behavior of algorithms.\n",
    "\n",
    "I like to look at the differences in the peaks as well as the spread or base of the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plots of accuracy\n",
    "scales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\n",
    "densityplot(results, scales=scales, pch = \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dot Plots** \n",
    "\n",
    "These are useful plots as the show both the mean estimated accuracy as well as the 95% confidence interval (e.g. the range in which 95% of observed scores fell).\n",
    "\n",
    "I find it useful to compare the means and eye-ball the overlap of the spreads between algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot plots of accuracy\n",
    "scales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\n",
    "dotplot(results, scales=scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Parallel Plots**\n",
    "\n",
    "This is another way to look at the data. It shows how each trial of each cross validation fold behaved for each of the algorithms tested. It can help you see how those hold-out subsets that were difficult for one algorithms faired for other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel plots to compare models\n",
    "parallelplot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scatterplot Matrix**\n",
    "\n",
    "This create a scatterplot matrix of all fold-trial results for an algorithm compared to the same fold-trial results for all other algorithms. All pairs are compared.\n",
    "\n",
    "This is invaluable when considering whether the predictions from two different algorithms are correlated. If weakly correlated, they are good candidates for being combined in an ensemble prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair-wise scatterplots of predictions to compare models\n",
    "splom(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pairwise xyPlots**\n",
    "\n",
    "You can zoom in on one pair-wise comparison of the accuracy of trial-folds for two machine learning algorithms with an xyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xyplot plots to compare models\n",
    "xyplot(results, models=c(\"LDA\", \"SVM\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Statistical Significance Tests**\n",
    "\n",
    "You can calculate the significance of the differences between the metric distributions of different machine learning algorithms. We can summarize the results directly by calling the summary() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in model predictions\n",
    "diffs <- diff(results)\n",
    "# summarize p-values for pair-wise comparisons\n",
    "summary(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Tuning\n",
    "Once we have found one or two algorithms that perform well on our dataset, we may want to improve the performance of those models.One way to increase the performance of an algorithm is to tune it’s parameters to our specific dataset.\n",
    "\n",
    "The caret package provides three ways to search for combinations of parameters for a machine learning algorithm.\n",
    "\n",
    "- Tune the parameters of an algorithm automatically (e.g. see the tuneLength argument to train()).\n",
    "- Tune the parameters of an algorithm using a grid search that we specify.\n",
    "- Tune the parameters of an algorithm using a random search.\n",
    "\n",
    "Take a look at the help for the **trainControl()** and **train()** functions and take note of the method and the tuneGrid arguments.\n",
    "\n",
    "*The Learning Vector Quantization (**LVQ**)* will be used in all examples because of its simplicity. It is like k-nearest neighbors, except the database of samples is smaller and adapted based on training data. It has two parameters to tune, the number of instances (codebooks) in the model called the size, and the number of instances to check when making predictions called k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-**Grid Search: Automatic Grid**\n",
    "There are two ways to tune an algorithm in the Caret R package, the first is by allowing the system to do it automatically. This can be done by setting the tuneLength to indicate the number of different values to try for each algorithm parameter.\n",
    "\n",
    "This only supports integer and categorical algorithm parameters, and it makes a crude guess as to what values to try, but it can get you up and running very quickly.\n",
    "\n",
    "-**Grid Search: Manual Grid**\n",
    "The second way to search algorithm parameters is to specify a tune grid manually. In the grid, each algorithm parameter can be specified as a vector of possible values. These vectors combine to define all the possible combinations to try.\n",
    "\n",
    "-**Data Pre-Processing**\n",
    "The dataset can be preprocessed as part of the parameter tuning. It is important to do this within the sample used to evaluate each model, to ensure that the results account for all the variability in the test. If the data set say normalized or standardized before the tuning process, it would have access to additional knowledge (bias) and not give an as accurate estimate of performance on unseen data.\n",
    "\n",
    "-**Parallel Processing**\n",
    "The caret package supports parallel processing in order to decrease the compute time for a given experiment. It is supported automatically as long as it is configured.\n",
    "\n",
    "-**Visualization of Performance**\n",
    "It can be useful to graph the performance of different algorithm parameter combinations to look for trends and the sensitivity of the model. Caret supports graphing the model directly which will compare the accuracy of different algorithm combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "library(caret)\n",
    "# load the iris dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# define training control\n",
    "trainControl <- trainControl(method=\"cv\", number=10)\n",
    "# define a grid of parameters to search for random forest\n",
    "grid <- expand.grid(.mtry=c(1,2,3,4,5,6,7,8,10))\n",
    "# estimate the accuracy of Random Forest on the dataset\n",
    "fit <- train(diabetes~., data=PimaIndiansDiabetes, trControl=trainControl, tuneGrid=grid, method=\"rf\")\n",
    "# summarize the estimated accuracy\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random Forest \n",
    "\n",
    "768 samples\n",
    "  8 predictor\n",
    "  2 classes: 'neg', 'pos' \n",
    "\n",
    "No pre-processing\n",
    "Resampling: Cross-Validated (10 fold) \n",
    "Summary of sample sizes: 691, 691, 691, 691, 692, 691, ... \n",
    "Resampling results across tuning parameters:\n",
    "\n",
    "  mtry  Accuracy   Kappa    \n",
    "   1    0.7511620  0.4236067\n",
    "   2    0.7706425  0.4809448\n",
    "   3    0.7666781  0.4797696\n",
    "   4    0.7602358  0.4604994\n",
    "   5    0.7654648  0.4746157\n",
    "   6    0.7628332  0.4691105\n",
    "   7    0.7628332  0.4656115\n",
    "   8    0.7628674  0.4666438\n",
    "  10    0.7641661  0.4708714\n",
    "\n",
    "Accuracy was used to select the optimal model using the largest value.\n",
    "The final value used for the model was mtry = 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automatic Grid\n",
    "\n",
    "The following recipe demonstrates the automatic grid search of the size and k attributes of LVQ with 5 (tuneLength=5) values of each (25 total models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure results are repeatable\n",
    "set.seed(7)\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# prepare training scheme\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, method=\"lvq\", trControl=control, tuneLength=5)\n",
    "# summarize the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manual Grid\n",
    "\n",
    "The recipe below demonstrates the search of a manual tune grid with 4 values for the size parameter and 5 values for the k parameter (20 combinations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure results are repeatable\n",
    "set.seed(7)\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# prepare training scheme\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "# design the parameter tuning grid\n",
    "grid <- expand.grid(size=c(5,10,20,50), k=c(1,2,3,4,5))\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, method=\"lvq\", trControl=control, tuneGrid=grid)\n",
    "# summarize the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Pre-Processing\n",
    "\n",
    "The attributes in the iris dataset are all in the same units and generally the same scale, so normalization and standardization are not really necessary. Nevertheless, the example below demonstrates tuning the size and k parameters of LVQ while normalizing the dataset with preProcess=”scale”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure results are repeatable\n",
    "set.seed(7)\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# prepare training scheme\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, method=\"lvq\", preProcess=\"scale\", trControl=control, tuneLength=5)\n",
    "# summarize the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parallel Processing\n",
    "\n",
    "In this example we load the doMC package and set the number of cores to 4, making available 4 worker threads to caret when tuning the model. This is used for the loops for the repeats of cross validation for each parameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure results are repeatable\n",
    "set.seed(7)\n",
    "# configure multicore\n",
    "library(doMC)\n",
    "registerDoMC(cores=4)\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# prepare training scheme\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, method=\"lvq\", trControl=control, tuneLength=5)\n",
    "# summarize the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualization of Performance\n",
    "\n",
    " In the recipe below, a larger manual grid of algorithm parameters are defined and the results are graphed. The graph shows the size on the x axis and model accuracy on the y axis. Two lines are drawn, one for each k value. The graph shows the general trends in the increase in performance with size and that the larger value of k is probably preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure results are repeatable\n",
    "set.seed(7)\n",
    "# load the library\n",
    "library(caret)\n",
    "# load the dataset\n",
    "data(iris)\n",
    "# prepare training scheme\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "# design the parameter tuning grid\n",
    "grid <- expand.grid(size=c(5,10,15,20,25,30,35,40,45,50), k=c(3,5))\n",
    "# train the model\n",
    "model <- train(Species~., data=iris, method=\"lvq\", trControl=control, tuneGrid=grid)\n",
    "# summarize the model\n",
    "print(model)\n",
    "# plot the effect of parameters on accuracy\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Predictions\n",
    "Another way that we can improve the performance of our models is to combine the predictions from multiple models.\n",
    "\n",
    "Some models provide this capability built-in such as **random forest** for *bagging* and **stochastic gradient boosting** for *boosting*. Another type of ensembling called **stacking (or blending)** can learn how to best combine the predictions from multiple models and is provided in the package *caretEnsemble.*\n",
    "\n",
    "- Bagging ensembles with the random forest and bagged CART algorithms in caret.\n",
    "- Boosting ensembles with the gradient boosting machine and C5.0 algorithms in caret.\n",
    "- Stacking ensembles using the caretEnsemble package and the caretStack() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-Combine Model Predictions Into Ensemble Predictions**\n",
    "\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "- ***Bagging.*** Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "- ***Boosting.*** Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.\n",
    "- ***Stacking.*** Building multiple models (typically of differing types) and supervisor model that learns how to best combine the predictions of the primary models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "library(mlbench)\n",
    "library(caret)\n",
    "library(caretEnsemble)\n",
    "# load the Pima Indians Diabetes dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# create sub-models\n",
    "trainControl <- trainControl(method=\"cv\", number=5, savePredictions=TRUE, classProbs=TRUE)\n",
    "algorithmList <- c('knn', 'glm')\n",
    "set.seed(7)\n",
    "models <- caretList(diabetes~., data=PimaIndiansDiabetes, trControl=trainControl, methodList=algorithmList)\n",
    "print(models)\n",
    "# learn how to best combine the predictions\n",
    "stackControl <- trainControl(method=\"cv\", number=5, savePredictions=TRUE, classProbs=TRUE)\n",
    "set.seed(7)\n",
    "stack.glm <- caretStack(models, method=\"glm\", trControl=stackControl)\n",
    "print(stack.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataset\n",
    "\n",
    "This dataset describes high-frequency antenna returns from high energy particles in the atmosphere and whether the return shows structure or not. The problem is a binary classification that contains 351 instances and 35 numerical attributes.\n",
    "\n",
    "Note that the first attribute was a factor (0,1) and has been transformed to be numeric for consistency with all of the other numeric attributes. Also note that the second attribute is a constant and has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "library(mlbench)\n",
    "library(caret)\n",
    "library(caretEnsemble)\n",
    "\n",
    "# Load the dataset\n",
    "data(Ionosphere)\n",
    "dataset <- Ionosphere\n",
    "dataset <- dataset[,-2]\n",
    "dataset$V1 <- as.numeric(as.character(dataset$V1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Boosting Algorithms**\n",
    "\n",
    "We can look at two of the most popular boosting machine learning algorithms:\n",
    "\n",
    "- C5.0\n",
    "- Stochastic Gradient Boosting\n",
    "\n",
    "Below is an example of the C5.0 and Stochastic Gradient Boosting (using the Gradient Boosting Modeling implementation) algorithms in R. Both algorithms include parameters that are not tuned in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Boosting Algorithms\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "seed <- 7\n",
    "metric <- \"Accuracy\"\n",
    "# C5.0\n",
    "set.seed(seed)\n",
    "fit.c50 <- train(Class~., data=dataset, method=\"C5.0\", metric=metric, trControl=control)\n",
    "# Stochastic Gradient Boosting\n",
    "set.seed(seed)\n",
    "fit.gbm <- train(Class~., data=dataset, method=\"gbm\", metric=metric, trControl=control, verbose=FALSE)\n",
    "# summarize results\n",
    "boosting_results <- resamples(list(c5.0=fit.c50, gbm=fit.gbm))\n",
    "summary(boosting_results)\n",
    "dotplot(boosting_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Bagging Algorithms**\n",
    "\n",
    "Let’s look at two of the most popular bagging machine learning algorithms:\n",
    "\n",
    "- Bagged CART\n",
    "- Random Forest\n",
    "\n",
    "Below is an example of the Bagged CART and Random Forest algorithms in R. Both algorithms include parameters that are not tuned in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Bagging algorithms\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "seed <- 7\n",
    "metric <- \"Accuracy\"\n",
    "# Bagged CART\n",
    "set.seed(seed)\n",
    "fit.treebag <- train(Class~., data=dataset, method=\"treebag\", metric=metric, trControl=control)\n",
    "# Random Forest\n",
    "set.seed(seed)\n",
    "fit.rf <- train(Class~., data=dataset, method=\"rf\", metric=metric, trControl=control)\n",
    "# summarize results\n",
    "bagging_results <- resamples(list(treebag=fit.treebag, rf=fit.rf))\n",
    "summary(bagging_results)\n",
    "dotplot(bagging_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Stacking Algorithms**\n",
    "\n",
    "You can combine the predictions of multiple caret models using the caretEnsemble package.\n",
    "\n",
    "Given a list of caret models, the caretStack() function can be used to specify a higher-order model to learn how to best combine the predictions of sub-models together.\n",
    "\n",
    "Let’s first look at creating 5 sub-models for the ionosphere dataset, specifically:\n",
    "\n",
    "- Linear Discriminate Analysis (LDA)\n",
    "- Classification and Regression Trees (CART)\n",
    "- Logistic Regression (via Generalized Linear Model or GLM)\n",
    "- k-Nearest Neighbors (kNN)\n",
    "- Support Vector Machine with a Radial Basis Kernel Function (SVM)\n",
    "\n",
    "Below is an example that creates these 5 sub-models. Note the new helpful caretList() function provided by the caretEnsemble package for creating a list of standard caret models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Stacking algorithms\n",
    "# create submodels\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)\n",
    "algorithmList <- c('lda', 'rpart', 'glm', 'knn', 'svmRadial')\n",
    "set.seed(seed)\n",
    "models <- caretList(Class~., data=dataset, trControl=control, methodList=algorithmList)\n",
    "results <- resamples(models)\n",
    "summary(results)\n",
    "dotplot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the predictions for the sub-models were highly corrected (>0.75) then they would be making the same or very similar predictions most of the time reducing the benefit of combining the predictions.\n",
    "\n",
    "Here, The two methods with the highest correlation between their predictions are Logistic Regression (GLM) and kNN at 0.517 correlation which is not considered high (>0.75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between results\n",
    "modelCor(results)\n",
    "splom(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s combine the predictions of the classifiers using a simple linear model.\n",
    "\n",
    "We have lifted the accuracy to 94.99% which is a small improvement over using SVM alone. This is also an improvement over using random forest alone on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack using glm\n",
    "stackControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)\n",
    "set.seed(seed)\n",
    "stack.glm <- caretStack(models, method=\"glm\", metric=\"Accuracy\", trControl=stackControl)\n",
    "print(stack.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use more sophisticated algorithms to combine predictions in an effort to tease out when best to use the different methods. In this case, we can use the random forest algorithm to combine the predictions.\n",
    "\n",
    "This has lifted the accuracy to 96.26% an impressive improvement on SVM alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack using random forest\n",
    "set.seed(seed)\n",
    "stack.rf <- caretStack(models, method=\"rf\", metric=\"Accuracy\", trControl=stackControl)\n",
    "print(stack.rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize And Save Model\n",
    "The tasks related to finalizing our model.\n",
    "\n",
    "- Using the predict() function to make predictions with a model trained using caret.\n",
    "- Training standalone versions of well performing models.\n",
    "- Saving trained models to file and loading them up again using the saveRDS() and readRDS() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load package\n",
    "library(randomForest)\n",
    "# load iris data\n",
    "data(PimaIndiansDiabetes)\n",
    "# train random forest model\n",
    "finalModel <- randomForest(diabetes~., PimaIndiansDiabetes, mtry=2, ntree=2000)\n",
    "# display the details of the final model\n",
    "print(finalModel)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Call:\n",
    " randomForest(formula = diabetes ~ ., data = PimaIndiansDiabetes,      mtry = 2, ntree = 2000) \n",
    "               Type of random forest: classification\n",
    "                     Number of trees: 2000\n",
    "No. of variables tried at each split: 2\n",
    "\n",
    "        OOB estimate of  error rate: 23.05%\n",
    "Confusion matrix:\n",
    "    neg pos class.error\n",
    "neg 427  73   0.1460000\n",
    "pos 104 164   0.3880597"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Make Predictions On New Data**\n",
    "\n",
    "You can make new predictions using a model you have tuned using caret using the predict.train() function.\n",
    "\n",
    "In the recipe below, the dataset is split into a validation dataset and a training dataset. The validation dataset could just as easily be a new dataset stored in a separate file and loaded as a data frame.\n",
    "\n",
    "A good model of the data is found using LDA. We can see that caret provides access to the best model from a training run in the finalModel variable.\n",
    "\n",
    "We can use that model to make predictions by calling predict using the fit from train which will automatically use the final model. We must specify the data one which to make predictions via the newdata argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "# load dataset\n",
    "data(PimaIndiansDiabetes)\n",
    "# create 80%/20% for training and validation datasets\n",
    "set.seed(9)\n",
    "validation_index <- createDataPartition(PimaIndiansDiabetes$diabetes, p=0.80, list=FALSE)\n",
    "validation <- PimaIndiansDiabetes[-validation_index,]\n",
    "training <- PimaIndiansDiabetes[validation_index,]\n",
    "# train a model and summarize model\n",
    "set.seed(9)\n",
    "control <- trainControl(method=\"cv\", number=10)\n",
    "fit.lda <- train(diabetes~., data=training, method=\"lda\", metric=\"Accuracy\", trControl=control)\n",
    "print(fit.lda)\n",
    "print(fit.lda$finalModel)\n",
    "# estimate skill on validation dataset\n",
    "set.seed(9)\n",
    "predictions <- predict(fit.lda, newdata=validation)\n",
    "confusionMatrix(predictions, validation$diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create A Standalone Model**\n",
    "\n",
    "In this example, we have tuned a random forest with 3 different values for mtry and ntree set to 2000. By printing the fit and the finalModel, we can see that the most accurate value for mtry was 2.\n",
    "\n",
    "Now that we know a good algorithm (random forest) and the good configuration (mtry=2, ntree=2000) we can create the final model directly using all of the training data. We can lookup the “rf” random forest implementation used by caret in the Caret List of Models and note that it is using the randomForest package and in turn the randomForest() function.\n",
    "\n",
    "The example creates a new model directly and uses it to make predictions on the new data, this case simulated as the verification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "library(randomForest)\n",
    "# load dataset\n",
    "data(Sonar)\n",
    "set.seed(7)\n",
    "# create 80%/20% for training and validation datasets\n",
    "validation_index <- createDataPartition(Sonar$Class, p=0.80, list=FALSE)\n",
    "validation <- Sonar[-validation_index,]\n",
    "training <- Sonar[validation_index,]\n",
    "# train a model and summarize model\n",
    "set.seed(7)\n",
    "control <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n",
    "fit.rf <- train(Class~., data=training, method=\"rf\", metric=\"Accuracy\", trControl=control, ntree=2000)\n",
    "print(fit.rf)\n",
    "print(fit.rf$finalModel)\n",
    "# create standalone model using all training data\n",
    "set.seed(7)\n",
    "finalModel <- randomForest(Class~., training, mtry=2, ntree=2000)\n",
    "# make a predictions on \"new data\" using the final model\n",
    "final_predictions <- predict(finalModel, validation[,1:60])\n",
    "confusionMatrix(final_predictions, validation$Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Save and Load Your Model**\n",
    "\n",
    "You can save your best models to a file so that you can load them up later and make predictions.\n",
    "\n",
    "In this example we split the Sonar dataset into a training dataset and a validation dataset. We take our validation dataset as new data to test our final model. We train the final model using the training dataset and our optimal parameters, then save it to a file called final_model.rds in the local working directory.\n",
    "\n",
    "The model is serialized. It can be loaded at a later time by calling readRDS() and assigning the object that is loaded (in this case a random forest fit) to a variable name. The loaded random forest is then used to make predictions on new data, in this case the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "library(randomForest)\n",
    "library(doMC)\n",
    "registerDoMC(cores=8)\n",
    "# load dataset\n",
    "data(Sonar)\n",
    "set.seed(7)\n",
    "# create 80%/20% for training and validation datasets\n",
    "validation_index <- createDataPartition(Sonar$Class, p=0.80, list=FALSE)\n",
    "validation <- Sonar[-validation_index,]\n",
    "training <- Sonar[validation_index,]\n",
    "# create final standalone model using all training data\n",
    "set.seed(7)\n",
    "final_model <- randomForest(Class~., training, mtry=2, ntree=2000)\n",
    "# save the model to disk\n",
    "saveRDS(final_model, \"./final_model.rds\")\n",
    "\n",
    "# later...\n",
    "\n",
    "# load the model\n",
    "super_model <- readRDS(\"./final_model.rds\")\n",
    "print(super_model)\n",
    "# make a predictions on \"new data\" using the final model\n",
    "final_predictions <- predict(super_model, validation[,1:60])\n",
    "confusionMatrix(final_predictions, validation$Class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
