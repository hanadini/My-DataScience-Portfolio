{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM RNN (with keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to use a favorite book from childhood as the dataset: Alice’s Adventures in Wonderland by Lewis Carroll.\n",
    "\n",
    "We are going to learn the dependencies between characters and the conditional probabilities of characters in sequences so that we can in turn generate wholly new and original sequences of characters.\n",
    "\n",
    "Let’s start off by importing the classes and functions we intend to use to train our model.\n",
    "\n",
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn.\n",
    "\n",
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer.\n",
    "\n",
    "or example, the list of unique sorted lowercase characters in the book is as follows:\n",
    "\n",
    "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xbb', '\\xbf', '\\xef']\n",
    "\n",
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset.\n",
    "\n",
    "We can see that the book has just under 150,000 characters and that when converted to lowercase that there are only 47 distinct characters in the vocabulary for the network to learn. Much more than the 26 in the alphabet.\n",
    "\n",
    "we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "\n",
    "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course).\n",
    "\n",
    "As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier.\n",
    "\n",
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.\n",
    "\n",
    "We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1.\n",
    "\n",
    "The problem is really a single character classification problem with 47 classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed.\n",
    "\n",
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
    "\n",
    "We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n",
    "\n",
    "The network is slow to train (about 300 seconds per epoch on an Nvidia K520 GPU). Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section.\n",
    "\n",
    "We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144435\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144335\n",
      "Epoch 1/20\n",
      "144335/144335 [==============================] - 807s 6ms/step - loss: 2.9709\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.97088, saving model to weights-improvement-01-2.9709.hdf5\n",
      "Epoch 2/20\n",
      "144335/144335 [==============================] - 834s 6ms/step - loss: 2.7780\n",
      "\n",
      "Epoch 00002: loss improved from 2.97088 to 2.77797, saving model to weights-improvement-02-2.7780.hdf5\n",
      "Epoch 3/20\n",
      "144335/144335 [==============================] - 844s 6ms/step - loss: 2.6802\n",
      "\n",
      "Epoch 00003: loss improved from 2.77797 to 2.68016, saving model to weights-improvement-03-2.6802.hdf5\n",
      "Epoch 4/20\n",
      "144335/144335 [==============================] - 834s 6ms/step - loss: 2.5988\n",
      "\n",
      "Epoch 00004: loss improved from 2.68016 to 2.59877, saving model to weights-improvement-04-2.5988.hdf5\n",
      "Epoch 5/20\n",
      "144335/144335 [==============================] - 831s 6ms/step - loss: 2.5360\n",
      "\n",
      "Epoch 00005: loss improved from 2.59877 to 2.53604, saving model to weights-improvement-05-2.5360.hdf5\n",
      "Epoch 6/20\n",
      "144335/144335 [==============================] - 831s 6ms/step - loss: 2.4763\n",
      "\n",
      "Epoch 00006: loss improved from 2.53604 to 2.47631, saving model to weights-improvement-06-2.4763.hdf5\n",
      "Epoch 7/20\n",
      "144335/144335 [==============================] - 838s 6ms/step - loss: 2.4198\n",
      "\n",
      "Epoch 00007: loss improved from 2.47631 to 2.41983, saving model to weights-improvement-07-2.4198.hdf5\n",
      "Epoch 8/20\n",
      "144335/144335 [==============================] - 837s 6ms/step - loss: 2.3700\n",
      "\n",
      "Epoch 00008: loss improved from 2.41983 to 2.36997, saving model to weights-improvement-08-2.3700.hdf5\n",
      "Epoch 9/20\n",
      "144335/144335 [==============================] - 830s 6ms/step - loss: 2.3243\n",
      "\n",
      "Epoch 00009: loss improved from 2.36997 to 2.32432, saving model to weights-improvement-09-2.3243.hdf5\n",
      "Epoch 10/20\n",
      "144335/144335 [==============================] - 831s 6ms/step - loss: 2.2807\n",
      "\n",
      "Epoch 00010: loss improved from 2.32432 to 2.28067, saving model to weights-improvement-10-2.2807.hdf5\n",
      "Epoch 11/20\n",
      "144335/144335 [==============================] - 833s 6ms/step - loss: 2.2420\n",
      "\n",
      "Epoch 00011: loss improved from 2.28067 to 2.24201, saving model to weights-improvement-11-2.2420.hdf5\n",
      "Epoch 12/20\n",
      "144335/144335 [==============================] - 835s 6ms/step - loss: 2.2024\n",
      "\n",
      "Epoch 00012: loss improved from 2.24201 to 2.20244, saving model to weights-improvement-12-2.2024.hdf5\n",
      "Epoch 13/20\n",
      "144335/144335 [==============================] - 837s 6ms/step - loss: 2.1649\n",
      "\n",
      "Epoch 00013: loss improved from 2.20244 to 2.16487, saving model to weights-improvement-13-2.1649.hdf5\n",
      "Epoch 14/20\n",
      "144335/144335 [==============================] - 838s 6ms/step - loss: 2.1306\n",
      "\n",
      "Epoch 00014: loss improved from 2.16487 to 2.13062, saving model to weights-improvement-14-2.1306.hdf5\n",
      "Epoch 15/20\n",
      "144335/144335 [==============================] - 838s 6ms/step - loss: 2.0966\n",
      "\n",
      "Epoch 00015: loss improved from 2.13062 to 2.09662, saving model to weights-improvement-15-2.0966.hdf5\n",
      "Epoch 16/20\n",
      "144335/144335 [==============================] - 838s 6ms/step - loss: 2.0637\n",
      "\n",
      "Epoch 00016: loss improved from 2.09662 to 2.06369, saving model to weights-improvement-16-2.0637.hdf5\n",
      "Epoch 17/20\n",
      "144335/144335 [==============================] - 834s 6ms/step - loss: 2.0313\n",
      "\n",
      "Epoch 00017: loss improved from 2.06369 to 2.03127, saving model to weights-improvement-17-2.0313.hdf5\n",
      "Epoch 18/20\n",
      "144335/144335 [==============================] - 836s 6ms/step - loss: 2.0042\n",
      "\n",
      "Epoch 00018: loss improved from 2.03127 to 2.00418, saving model to weights-improvement-18-2.0042.hdf5\n",
      "Epoch 19/20\n",
      "144335/144335 [==============================] - 835s 6ms/step - loss: 1.9774\n",
      "\n",
      "Epoch 00019: loss improved from 2.00418 to 1.97737, saving model to weights-improvement-19-1.9774.hdf5\n",
      "Epoch 20/20\n",
      "144335/144335 [==============================] - 837s 6ms/step - loss: 1.9505\n",
      "\n",
      "Epoch 00020: loss improved from 1.97737 to 1.95049, saving model to weights-improvement-20-1.9505.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x78a34a288>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"C:\\\\Users\\\\user\\\\Desktop\\\\ML\\\\Data\\\\wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with an LSTM Network\n",
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained.\n",
    "\n",
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions.\n",
    "\n",
    "Finally, we need to actually make predictions.\n",
    "\n",
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144435\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144335\n",
      "Seed:\n",
      "\" mous puppy was looking down at her with large round eyes, and\n",
      "feebly stretching out one paw, trying  \"\n",
      "to the white rabbit  whth a siile tas oo aaainen to be ince, and the whrte tabbit retee oo aalit an inc thte the was so aeyir the rooe of the care \n",
      "and the world ser at the cadk an in wpth a lottle oo the sooeo oh the care and the san oo sie whrl on  'in m sar an the cruloer ' said the manch hare.\n",
      "\n",
      "'i d note she toiet hare toted hoo toeerinn,' said the manch hare.\n",
      "\n",
      "'i d note she toiet hare toted hoo tore   said the mock turtle, 'the wert oo toe that to tey, that s ao in  \n",
      "\n",
      "'i don't keve the hrrse totld ' said the monk turtle, and the terten of then on the was oo a aitt of the samee of the carere aaai to the coore, and the whrte tarbe an the call an in wouh a lottle  and the sooe of the sore of the carerpillar teet iar aadin to tee whrt gerden the har hnn the whrte tar an inc thite tf the thoe the har hnr sar the karge hare, and she sam oo aliteer siee an ierself  bnd the sooe of the sooeo oo the sabbit har so the korth  and the wert oelele oo the care afdin, and the seiter oa toole the\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"C:\\\\Users\\\\user\\\\Desktop\\\\ML\\\\Data\\\\wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-20-1.9505.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note some observations about the generated text.\n",
    "\n",
    "It generally conforms to the line format observed in the original text of less than 80 characters before a new line.\n",
    "The characters are separated into word-like groups and most groups are actual English words (e.g. “the”, “little” and “was”), but many do not (e.g. “lott”, “tiie” and “taede”).\n",
    "Some of the words in sequence make sense(e.g. “and the white rabbit“), but many do not (e.g. “wese tilel“)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger LSTM RNN\n",
    "\n",
    "We got results, but not excellent results in the previous section. Now, we can try to improve the quality of the generated text by creating a much larger network.\n",
    "\n",
    "We will keep the number of memory units the same at 256, but add a second layer.\n",
    "\n",
    "We will also change the filename of the checkpointed weights so that we can tell the difference between weights for this network and the previous (by appending the word “bigger” in the filename).\n",
    "\n",
    "Finally, we will increase the number of training epochs from 20 to 50 and decrease the batch size from 128 to 64 to give the network more of an opportunity to be updated and learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144435\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144335\n",
      "Epoch 1/50\n",
      "144335/144335 [==============================] - 2435s 17ms/step - loss: 2.7945\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.79448, saving model to weights-improvement-01-2.7945-bigger.hdf5\n",
      "Epoch 2/50\n",
      "144335/144335 [==============================] - 2596s 18ms/step - loss: 2.4369\n",
      "\n",
      "Epoch 00002: loss improved from 2.79448 to 2.43691, saving model to weights-improvement-02-2.4369-bigger.hdf5\n",
      "Epoch 3/50\n",
      "144335/144335 [==============================] - 2477s 17ms/step - loss: 2.2323\n",
      "\n",
      "Epoch 00003: loss improved from 2.43691 to 2.23229, saving model to weights-improvement-03-2.2323-bigger.hdf5\n",
      "Epoch 4/50\n",
      "144335/144335 [==============================] - 2468s 17ms/step - loss: 2.0927\n",
      "\n",
      "Epoch 00004: loss improved from 2.23229 to 2.09275, saving model to weights-improvement-04-2.0927-bigger.hdf5\n",
      "Epoch 5/50\n",
      "144335/144335 [==============================] - 2463s 17ms/step - loss: 1.9923\n",
      "\n",
      "Epoch 00005: loss improved from 2.09275 to 1.99226, saving model to weights-improvement-05-1.9923-bigger.hdf5\n",
      "Epoch 6/50\n",
      "144335/144335 [==============================] - 2464s 17ms/step - loss: 1.9127\n",
      "\n",
      "Epoch 00006: loss improved from 1.99226 to 1.91268, saving model to weights-improvement-06-1.9127-bigger.hdf5\n",
      "Epoch 7/50\n",
      "144335/144335 [==============================] - 2462s 17ms/step - loss: 1.8512\n",
      "\n",
      "Epoch 00007: loss improved from 1.91268 to 1.85120, saving model to weights-improvement-07-1.8512-bigger.hdf5\n",
      "Epoch 8/50\n",
      "144335/144335 [==============================] - 2462s 17ms/step - loss: 1.7974\n",
      "\n",
      "Epoch 00008: loss improved from 1.85120 to 1.79741, saving model to weights-improvement-08-1.7974-bigger.hdf5\n",
      "Epoch 9/50\n",
      "144335/144335 [==============================] - 2460s 17ms/step - loss: 1.7478\n",
      "\n",
      "Epoch 00009: loss improved from 1.79741 to 1.74779, saving model to weights-improvement-09-1.7478-bigger.hdf5\n",
      "Epoch 10/50\n",
      "144335/144335 [==============================] - 2459s 17ms/step - loss: 1.7051\n",
      "\n",
      "Epoch 00010: loss improved from 1.74779 to 1.70511, saving model to weights-improvement-10-1.7051-bigger.hdf5\n",
      "Epoch 11/50\n",
      "144335/144335 [==============================] - 2462s 17ms/step - loss: 1.6711\n",
      "\n",
      "Epoch 00011: loss improved from 1.70511 to 1.67112, saving model to weights-improvement-11-1.6711-bigger.hdf5\n",
      "Epoch 12/50\n",
      "144335/144335 [==============================] - 2456s 17ms/step - loss: 1.6311\n",
      "\n",
      "Epoch 00012: loss improved from 1.67112 to 1.63108, saving model to weights-improvement-12-1.6311-bigger.hdf5\n",
      "Epoch 13/50\n",
      "144335/144335 [==============================] - 2453s 17ms/step - loss: 1.6033\n",
      "\n",
      "Epoch 00013: loss improved from 1.63108 to 1.60326, saving model to weights-improvement-13-1.6033-bigger.hdf5\n",
      "Epoch 14/50\n",
      "144335/144335 [==============================] - 2450s 17ms/step - loss: 1.5710\n",
      "\n",
      "Epoch 00014: loss improved from 1.60326 to 1.57100, saving model to weights-improvement-14-1.5710-bigger.hdf5\n",
      "Epoch 15/50\n",
      "144335/144335 [==============================] - 2450s 17ms/step - loss: 1.5463\n",
      "\n",
      "Epoch 00015: loss improved from 1.57100 to 1.54633, saving model to weights-improvement-15-1.5463-bigger.hdf5\n",
      "Epoch 16/50\n",
      "144335/144335 [==============================] - 2452s 17ms/step - loss: 1.5191\n",
      "\n",
      "Epoch 00016: loss improved from 1.54633 to 1.51913, saving model to weights-improvement-16-1.5191-bigger.hdf5\n",
      "Epoch 17/50\n",
      "144335/144335 [==============================] - 2450s 17ms/step - loss: 1.4983\n",
      "\n",
      "Epoch 00017: loss improved from 1.51913 to 1.49832, saving model to weights-improvement-17-1.4983-bigger.hdf5\n",
      "Epoch 18/50\n",
      "144335/144335 [==============================] - 2452s 17ms/step - loss: 1.4736\n",
      "\n",
      "Epoch 00018: loss improved from 1.49832 to 1.47365, saving model to weights-improvement-18-1.4736-bigger.hdf5\n",
      "Epoch 19/50\n",
      "144335/144335 [==============================] - 2452s 17ms/step - loss: 1.4561\n",
      "\n",
      "Epoch 00019: loss improved from 1.47365 to 1.45609, saving model to weights-improvement-19-1.4561-bigger.hdf5\n",
      "Epoch 20/50\n",
      "144335/144335 [==============================] - 2456s 17ms/step - loss: 1.4368\n",
      "\n",
      "Epoch 00020: loss improved from 1.45609 to 1.43679, saving model to weights-improvement-20-1.4368-bigger.hdf5\n",
      "Epoch 21/50\n",
      "144335/144335 [==============================] - 2447s 17ms/step - loss: 1.4202\n",
      "\n",
      "Epoch 00021: loss improved from 1.43679 to 1.42025, saving model to weights-improvement-21-1.4202-bigger.hdf5\n",
      "Epoch 22/50\n",
      "144335/144335 [==============================] - 2443s 17ms/step - loss: 1.4056\n",
      "\n",
      "Epoch 00022: loss improved from 1.42025 to 1.40556, saving model to weights-improvement-22-1.4056-bigger.hdf5\n",
      "Epoch 23/50\n",
      "144335/144335 [==============================] - 2446s 17ms/step - loss: 1.3910\n",
      "\n",
      "Epoch 00023: loss improved from 1.40556 to 1.39105, saving model to weights-improvement-23-1.3910-bigger.hdf5\n",
      "Epoch 24/50\n",
      "144335/144335 [==============================] - 2445s 17ms/step - loss: 1.3780\n",
      "\n",
      "Epoch 00024: loss improved from 1.39105 to 1.37801, saving model to weights-improvement-24-1.3780-bigger.hdf5\n",
      "Epoch 25/50\n",
      "144335/144335 [==============================] - 2449s 17ms/step - loss: 1.3631\n",
      "\n",
      "Epoch 00025: loss improved from 1.37801 to 1.36309, saving model to weights-improvement-25-1.3631-bigger.hdf5\n",
      "Epoch 26/50\n",
      "144335/144335 [==============================] - 2458s 17ms/step - loss: 1.3499\n",
      "\n",
      "Epoch 00026: loss improved from 1.36309 to 1.34994, saving model to weights-improvement-26-1.3499-bigger.hdf5\n",
      "Epoch 27/50\n",
      "144335/144335 [==============================] - 2458s 17ms/step - loss: 1.3376\n",
      "\n",
      "Epoch 00027: loss improved from 1.34994 to 1.33760, saving model to weights-improvement-27-1.3376-bigger.hdf5\n",
      "Epoch 28/50\n",
      "144335/144335 [==============================] - 2451s 17ms/step - loss: 1.3249\n",
      "\n",
      "Epoch 00028: loss improved from 1.33760 to 1.32493, saving model to weights-improvement-28-1.3249-bigger.hdf5\n",
      "Epoch 29/50\n",
      "144335/144335 [==============================] - 2487s 17ms/step - loss: 1.3167\n",
      "\n",
      "Epoch 00029: loss improved from 1.32493 to 1.31673, saving model to weights-improvement-29-1.3167-bigger.hdf5\n",
      "Epoch 30/50\n",
      "144335/144335 [==============================] - 2463s 17ms/step - loss: 1.3029\n",
      "\n",
      "Epoch 00030: loss improved from 1.31673 to 1.30286, saving model to weights-improvement-30-1.3029-bigger.hdf5\n",
      "Epoch 31/50\n",
      "144335/144335 [==============================] - 2526s 17ms/step - loss: 1.2990\n",
      "\n",
      "Epoch 00031: loss improved from 1.30286 to 1.29902, saving model to weights-improvement-31-1.2990-bigger.hdf5\n",
      "Epoch 32/50\n",
      "144335/144335 [==============================] - 2479s 17ms/step - loss: 1.2884\n",
      "\n",
      "Epoch 00032: loss improved from 1.29902 to 1.28839, saving model to weights-improvement-32-1.2884-bigger.hdf5\n",
      "Epoch 33/50\n",
      "144335/144335 [==============================] - 2454s 17ms/step - loss: 1.2828\n",
      "\n",
      "Epoch 00033: loss improved from 1.28839 to 1.28280, saving model to weights-improvement-33-1.2828-bigger.hdf5\n",
      "Epoch 34/50\n",
      "144335/144335 [==============================] - 2454s 17ms/step - loss: 1.2766\n",
      "\n",
      "Epoch 00034: loss improved from 1.28280 to 1.27665, saving model to weights-improvement-34-1.2766-bigger.hdf5\n",
      "Epoch 35/50\n",
      "144335/144335 [==============================] - 2456s 17ms/step - loss: 1.2700\n",
      "\n",
      "Epoch 00035: loss improved from 1.27665 to 1.27004, saving model to weights-improvement-35-1.2700-bigger.hdf5\n",
      "Epoch 36/50\n",
      "144335/144335 [==============================] - 2454s 17ms/step - loss: 1.2588\n",
      "\n",
      "Epoch 00036: loss improved from 1.27004 to 1.25875, saving model to weights-improvement-36-1.2588-bigger.hdf5\n",
      "Epoch 37/50\n",
      "144335/144335 [==============================] - 2458s 17ms/step - loss: 1.2556\n",
      "\n",
      "Epoch 00037: loss improved from 1.25875 to 1.25556, saving model to weights-improvement-37-1.2556-bigger.hdf5\n",
      "Epoch 38/50\n",
      "144335/144335 [==============================] - 2458s 17ms/step - loss: 1.2482\n",
      "\n",
      "Epoch 00038: loss improved from 1.25556 to 1.24815, saving model to weights-improvement-38-1.2482-bigger.hdf5\n",
      "Epoch 39/50\n",
      "144335/144335 [==============================] - 2473s 17ms/step - loss: 1.2450\n",
      "\n",
      "Epoch 00039: loss improved from 1.24815 to 1.24502, saving model to weights-improvement-39-1.2450-bigger.hdf5\n",
      "Epoch 40/50\n",
      "144335/144335 [==============================] - 2479s 17ms/step - loss: 1.2396\n",
      "\n",
      "Epoch 00040: loss improved from 1.24502 to 1.23957, saving model to weights-improvement-40-1.2396-bigger.hdf5\n",
      "Epoch 41/50\n",
      "144335/144335 [==============================] - 2487s 17ms/step - loss: 1.2329\n",
      "\n",
      "Epoch 00041: loss improved from 1.23957 to 1.23295, saving model to weights-improvement-41-1.2329-bigger.hdf5\n",
      "Epoch 42/50\n",
      "144335/144335 [==============================] - 2500s 17ms/step - loss: 1.2305\n",
      "\n",
      "Epoch 00042: loss improved from 1.23295 to 1.23046, saving model to weights-improvement-42-1.2305-bigger.hdf5\n",
      "Epoch 43/50\n",
      "144335/144335 [==============================] - 2513s 17ms/step - loss: 1.2260\n",
      "\n",
      "Epoch 00043: loss improved from 1.23046 to 1.22603, saving model to weights-improvement-43-1.2260-bigger.hdf5\n",
      "Epoch 44/50\n",
      "144335/144335 [==============================] - 2475s 17ms/step - loss: 1.2197\n",
      "\n",
      "Epoch 00044: loss improved from 1.22603 to 1.21966, saving model to weights-improvement-44-1.2197-bigger.hdf5\n",
      "Epoch 45/50\n",
      "144335/144335 [==============================] - 2481s 17ms/step - loss: 1.2115\n",
      "\n",
      "Epoch 00045: loss improved from 1.21966 to 1.21152, saving model to weights-improvement-45-1.2115-bigger.hdf5\n",
      "Epoch 46/50\n",
      "144335/144335 [==============================] - 2507s 17ms/step - loss: 1.2123\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.21152\n",
      "Epoch 47/50\n",
      "144335/144335 [==============================] - 2519s 17ms/step - loss: 1.2052\n",
      "\n",
      "Epoch 00047: loss improved from 1.21152 to 1.20521, saving model to weights-improvement-47-1.2052-bigger.hdf5\n",
      "Epoch 48/50\n",
      "144335/144335 [==============================] - 2493s 17ms/step - loss: 1.2061\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.20521\n",
      "Epoch 49/50\n",
      "144335/144335 [==============================] - 2455s 17ms/step - loss: 1.2011\n",
      "\n",
      "Epoch 00049: loss improved from 1.20521 to 1.20109, saving model to weights-improvement-49-1.2011-bigger.hdf5\n",
      "Epoch 50/50\n",
      "144335/144335 [==============================] - 2454s 17ms/step - loss: 1.1989\n",
      "\n",
      "Epoch 00050: loss improved from 1.20109 to 1.19891, saving model to weights-improvement-50-1.1989-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7c126c448>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"C:\\\\Users\\\\user\\\\Desktop\\\\ML\\\\Data\\\\wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example takes some time, at least 700 seconds per epoch.\n",
    "\n",
    "After running this example you may achieved a loss of about 1.2. For example the best result I achieved from running this model was stored in a checkpoint file with the name:\n",
    "\n",
    "    weights-improvement-50-1.1989-bigger.hdf5\n",
    "\n",
    "Achieving a loss of 1.1989 at epoch 50.\n",
    "\n",
    "As in the previous section, we can use this best model from the run to generate text.\n",
    "\n",
    "The only change we need to make to the text generation script from the previous section is in the specification of the network topology and from which file to seed the network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144435\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144335\n",
      "Seed:\n",
      "\" d then i'll tell you my\n",
      "history, and you'll understand why it is i hate cats and dogs.'\n",
      "\n",
      "it was high \"\n",
      " time the had a little bottle of the coor of the way of sight for a linute or two the was a little bottle of the coor and suoping again.\n",
      "\n",
      "'i mever want to she sea,' said the cat, and vanished the rueen of her head to keare the sopf of the had some of the sabbit, and then said 'the farden, i should think i mnoe of the same size of a lobeted of me a pueer-thape wou were mine a little biiver that she was salking to her to the was of a lomgnt to larg the dormouse was soo a long way of siles of the way of a pueer-the way of siles of the way of a pueer-the way of siles of the words as she was a little bottle of the coor as she was a little bottle of the coor and a large cand of the coor was off, and the white rabbit was selling to her ay the words as she was a little bottle of the coor as she was a little bottle of the coor as she was a little bottle of the coor and a large cand of the coor was off, and the white rabbit was selling to her ay the words as she was a little bottle of the coor a\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"C:\\\\Users\\\\user\\\\Desktop\\\\ML\\\\Data\\\\wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-50-1.1989-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
